import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.cluster import KMeans,MeanShift,estimate_bandwidth
from sklearn.preprocessing import LabelEncoder
from matplotlib import pyplot as plt
from itertools import cycle

recipe = pd.read_csv('recipeData.csv', encoding='latin')
#fills all NaNs with 0's
beer = recipe.fillna(0)

#converts all unique strings to unique numbers
labels, uniques = pd.factorize(beer['Style'])
#labels all unique strings to unique numbers in BrewMethod column
labelEncoder = LabelEncoder()
labelEncoder.fit(beer['BrewMethod'])
beer['BrewMethod'] = labelEncoder.transform(beer['BrewMethod'])
#Drops irrelevant columns, including column with alphanumerical data
data = beer.drop(['BeerID','Name','Style','URL','SugarScale','PrimingMethod','PrimingAmount'], axis=1)
#train-test-split
x_train, x_test, y_train, y_test = train_test_split(data, labels)

# fit a Naive Bayes model to the data
model = GaussianNB()
model.fit(x_train,y_train)

# make predictions
expected = y_test
predicted = model.predict(x_test)
class_num = len(np.unique(predicted))

#Report number of classifications found by naive bayes
print('The Naive Bayes algorithm identified', class_num, 'classifications')
print('--------------')
# summarize the fit of the model
print(classification_report(expected, predicted))
#print(confusion_matrix(expected, predicted))

#Heat map does not show properly with so many datapoints
#cm = pd.DataFrame(confusion_matrix(y_test, predicted))
#sns.heatmap(cm, annot=True)
#plt.xlabel("Predicted")
#plt.ylabel("Expected")
#plt.title('Accuracy Score: %.6f' %accuracy_score(y_test, predicted))
#plt.show()

print("---------")
#Estimate the number of clusters predicted by Mean shift

X = np.array(data)

bandwidth = estimate_bandwidth(X, n_samples=5000)

ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)

labels_unique = np.unique(ms.labels_)
n_clusters_ = len(labels_unique)

print("Estimated number of cluster using Mean shift:",  n_clusters_)

#Train KMeans by feeding it the dataset and number of clusters, then make the predictions
#Takes a very long time to prccess because it iterates through 70000+ data points 500 times
kmeans = KMeans(n_clusters=n_clusters_, algorithm='auto', max_iter=500)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
labels = kmeans.labels_
centers = kmeans.cluster_centers_
#print(np.unique(y_kmeans))

#Scatter plot of data set using cluster centers and predictions generated by KMeans
#Takes as long as the Mean shift if not longer...
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
plt.xlabel('Lablels')
plt.ylabel('Datapoints')
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

#Compare between the number of classifications found by Naive Bayes vs the number of clusters found by Mean Shift
print('In conclusion, Naive Bayes was able to identify', class_num,'classifications while Mean shift was able to identify', n_clusters_,'clusters.')
print('As expected, Naive bayes was able to identify almost all 175 classifications as it is a supervised algorithm to which labels were fed while training the model.'
      ' However, the number of clusters identified by mean shift was much less in comparison as it is an unsupervised algorithm.')

